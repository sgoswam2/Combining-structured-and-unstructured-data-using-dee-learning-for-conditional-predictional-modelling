{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Unstructured_structured_deep.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPOsJp9s9BS5hCKyUwWtja8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"cbhGDyEc34-z"},"source":["This script develops a Deep learning based on combination of structured fields and unstructured text fields for a conditional hierarchical classification model."]},{"cell_type":"code","metadata":{"id":"X2CRrZbcyzVf"},"source":["import OS import random\n","import pandis as pd \n","import numpy as np\n","import matplotlib.pyplot as plt \n","plt.style.use(\"ggplot\")\n","%matplotlib inline\n","\n","from tqdm import tqdm_notebook, tnrange \n","from itertools import chain\n","from sklearn.model_selection import train_test_split\n","\n","import tensorflow as tf\n","from keras.models import Model, load_model\n","from keras.layers import Input, BatchNormalization, Activation, Dense,Dropout, Reshape, UpSampling2D \n","from keras.layers.core import Lambda, RepeatVector, Reshape\n","from keras.layers.convolutional import Conv1D\n","\n","from keras.layers.pooling import MaxPoolinglD \n","from keras.layers.merge import concatenate, add\n","from keras.callbacks import EarlyStopping, ModelCheckpoint,ReduceLROnPlateau \n","from keras.optiaizers import Adam\n","from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img \n","from keras.layers import Dense\n","from keras.layers import Flatten \n","from keras.layers import LSTM\n","from keras.layers import Bidirectional\n","from keras.regularizers import 12\n","\n","tf. device(\"gpu:1\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9AC0-b9K__0m"},"source":["Here we have two unstructured texts input and one structured field input"]},{"cell_type":"code","metadata":{"id":"ItpqZ_bh9K1X"},"source":["def Unstructured_structured_net(unstructured_text1,unstructured_text2, structured, length_unstructured1,length_unstructured2,vocab_size_unstructured1,vocab_size_unstructured2):\n","  #encoder unstructured_text1\n","\n","  #channel 1 (Here we will first consider kernel size 2, i.e, consider embeddings for 2 word phrases)\n","  embedding1 = Embedding(vocab_size_unstructured1, 200)(unstructured_text1) ## 200 represents embedding size for each word\n","  conv1= Conv1D(filters=32, kernel_size=2, activation='relu')(embedding1) ## kernel_size=2 means we consider embedding for 2-word phrases\n","  drop1= Dropout(0.2)(conv1)\n","  pool1= MaxPooling1D(pool_size=2)(drop1)\n","  seq1= Bidirectional(LSTM(20, return_sequences=True), input_shape=(length_unstructured1-1,32))(pool1) ### sequential meaning from previous and future sentences\n","  seq2= Bidirectional(LSTM(20, return_sequences=True))(seq1)\n","  flat1= Flatten()(seq2)\n","\n","  #channel 2 (Here we will first consider kernel size 3, i.e, consider embeddings for 3 word phrases)\n","  embedding2 = Embedding(vocab_size_unstructured1, 200)(unstructured_text1) ## 200 represents embedding size for each word\n","  conv2= Conv1D(filters=32, kernel_size=3, activation='relu')(embedding2) ## kernel_size=2 means we consider embedding for 2-word phrases\n","  drop2= Dropout(0.2)(conv2)\n","  pool2= MaxPooling1D(pool_size=2)(drop2)\n","  seq3= Bidirectional(LSTM(20, return_sequences=True), input_shape=(length_unstructured2-2,32))(pool2) ### sequential meaning from previous and future sentences\n","  seq4= Bidirectional(LSTM(20, return_sequences=True))(seq3)\n","  flat2= Flatten()(seq4) \n","\n","  ### we could increase channels with larger kernel_size if we want meaning-embeddings for higher word phrases\n","\n","  ### merge the channel 1 and channel 2\n","  merged= concatenate([flat1, flat2])\n","  ### interpretation\n","  dense1= Dense(2000, activation='relu', kernel_regularizer=l2(0.001))(merged)\n","  dense2= Dense(200, activation='relu')(dense1)\n","\n","  #encoder unstructured_text2\n","  #In our case unstructured_text2 sequence length is much lesser than unstructured_text1. so we only take 2 word-phrases.\n","  embedding3 = Embedding(vocab_size_unstructured2, 50)(unstructured_text2) ## 50 represents embedding size for each word\n","  conv3= Conv1D(filters=16, kernel_size=2, activation='relu')(embedding3) ## kernel_size=2 means we consider embedding for 2-word phrases\n","  drop3= Dropout(0.2)(conv3)\n","  pool3= MaxPooling1D(pool_size=2)(drop3)\n","  flat3= Flatten()(pool3)\n","  dense3= Dense(50, activation='relu')(flat3)\n","\n","  ## here we merge dense1(unstructured_text1), dense2(unstructured_text1), structured field)\n","  u7=concatenate([dense2,dense3,structured],axis=1)\n","  u7=Reshape(1,200+50+14)(u7)# 14 is the number of structured fields\n","\n","  ### decoder path (independent output)\n","  f1= Flatten()(u7)\n","  d1= dense(100, activation='relu')(f1)\n","  drop4= Dropout(0.2)(d1)\n","  d2= dense(10, activation='relu')(drop4)\n","  drop5= Dropout(0.2)(d2)\n","  output1= Dense(1, activation='sigmoid', name='independent_output')(drop5)\n","\n","  ### decoder path (dependent output)\n","  f2= Flatten()(u7)\n","  d3= dense(100, activation='relu')(f2)\n","  p11= concatenate([d1,d3])\n","  drop6= Dropout(0.2)(p11)\n","  d4= dense(10, activation='relu')(drop6)\n","  p12= concatenate([d2,d4])\n","  drop7= Dropout(0.2)(p12)\n","  output2= Dense(3, activation='sigmoid', name='dependent_output')(drop7) ## multiclass dependent output (not mutually exclusive hence sigmoid activation)\n","\n","  model= Model(inputs=[unstructured_text1,unstructured_text2, structured], outputs=[output1,output2])\n","  return model\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yPySLt5xKeI8"},"source":["#### define loss function\n","\n","from keras import backend as K\n","def weighted_categorical_crossentropy(weights):\n","    \"\"\"\n","    https://gist.github.com/wassname/ce364fddfc8a025bfab4348cf5de852d\n","\n","    A weighted version of keras.objectives.categorical_crossentropy\n","    \n","    Variables:\n","        weights: numpy array of shape (C,) where C is the number of classes\n","    \n","    Usage:\n","        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n","        loss = weighted_categorical_crossentropy(weights)\n","        model.compile(loss=loss,optimizer='adam')\n","    \"\"\"\n","    \n","    weights = K.variable(weights)\n","        \n","    def loss(y_true, y_pred):\n","        # scale predictions so that the class probas of each sample sum to 1\n","        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n","        # clip to prevent NaN's and Inf's\n","        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n","        # calc\n","        loss = y_true * K.log(y_pred) * weights\n","        loss = -K.sum(loss, -1)\n","        return loss\n","    \n","    return loss\n","\n","\n","def weighted_binary_crossentropy(zero_weight, one_weight):\n","\n","    def weighted_binary_crossentropy(y_true, y_pred):\n","\n","        # Original binary crossentropy (see losses.py):\n","        # K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1)\n","\n","        # Calculate the binary crossentropy\n","        b_ce = K.binary_crossentropy(y_true, y_pred)\n","\n","        # Apply the weights\n","        weight_vector = y_true * one_weight + (1. - y_true) * zero_weight\n","        weighted_b_ce = weight_vector * b_ce\n","\n","        # Return the mean error\n","        return K.mean(weighted_b_ce)\n","\n","    return weighted_binary_crossentropy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J_b50V6aQUXK"},"source":["input1 = Input((length_unstructured1,), name='unstructured_text1')\n","input2 = Input((length_unstructured2,), name='unstructured_text2')\n","input3 = Input((14,), name='structured_fields')\n","model = Unstructured_structured_net(input1,input2,input3, length_unstructured1,length_unstructured2,vocab_size_unstructured1,vocab_size_unstructured2)\n","\n","weights1=np.array([1,1]) ### this weights each output of independent-output section equally\n","weights2=np.array([10,5,1]) ### this weights are according to priority for dependent-output\n","model.compile(loss={'independent_output': weighted_binary_crossentropy(weights1[0],weights1[1]), \n","                    'dependent_output': weighted_categorical_crossentropy(weights2)},\n","              loss_weights={'independent_output': 1.0,\n","                            'dependent_output': 1.0},\n","              optimizer='adam',\n","              metrics={'independent_output': 'accuracy', 'dependent_output': 'accuracy'})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ggJN5d1eTzIV"},"source":["callbacks = [\n","    EarlyStopping(monitor='val_loss',mode='min',patience=15, verbose=1),\n","    ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.00001, verbose=1),\n","    ModelCheckpoint('unstructured_structured_deep.h5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True)\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YF2afKmsUXns"},"source":["results = model.fit([unstructured_text1_train,unstructured_text2_train,sructured_train], {'independent_output': outputs1_train, 'dependent_output': outputs2_train}, batch_size=64, epochs=150, callbacks=callbacks,\\\n","                    validation_data=([unstructured_text1_valid,unstructured_text2_valid,sructured_valid], {'independent_output': outputs1_valid, 'dependent_output': outputs2_valid}))"],"execution_count":null,"outputs":[]}]}